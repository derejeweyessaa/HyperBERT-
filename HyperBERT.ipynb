{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3nN39zehLtk9v5QOPi3cO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derejeweyessaa/HyperBERT-/blob/main/HyperBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#....Below is a  Python code that integrates hyperbolic embeddings\n",
        "with a BERT-based model for an interpretable ICD coding task using\n",
        "PyTorch, Hugging Face's Transformers, and Poincaré....#"
      ],
      "metadata": {
        "id": "OAzs4b48Act_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#.......To integrate hyperbolic embeddings and BERT-based representations for the automatic ICD coding task, we need to perform the following steps:\n",
        "\n",
        "1. Preprocess the data: Prepare discharge summaries and associated ICD codes.\n",
        "\n",
        "\n",
        "2. Train hyperbolic embeddings on the hierarchical structure of ICD codes using Poincaré embeddings.\n",
        "\n",
        "\n",
        "3. Fine-tune a pre-trained BERT model on the discharge summaries for the downstream task of ICD coding.along with multi label classification\n",
        "\n",
        "\n",
        "4. Integrate hyperbolic embeddings and BERT-based representations by combining them in a joint model.\n",
        "\n",
        "\n",
        "5. Train the joint model on the training data.\n",
        "\n",
        "\n",
        "6. Evaluate the performance of the joint model on a separate validation or test dataset.\n",
        "7. Visualization for interpretability\n",
        "\n",
        "...#"
      ],
      "metadata": {
        "id": "r-zJASljGAV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pip! Install Transformers"
      ],
      "metadata": {
        "id": "oNea8xaYGbGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pip! Install torch"
      ],
      "metadata": {
        "id": "Q-rgFdLlIolS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pip! Install PoincareModel"
      ],
      "metadata": {
        "id": "ca14dJhIGhlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pip! Install geomstats"
      ],
      "metadata": {
        "id": "ZTO5Go5AGr72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghivtc7_FX8l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from poincare_embedding import PoincareModel\n",
        "from geomstats.optimization.optimizers import RiemannianAdam\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Preprocessing\n",
        "def preprocess_mimic_iv(data_path):\n",
        "    mimic_data = pd.read_csv(data_path)\n",
        "    mimic_data = mimic_data[['discharge_summary', 'icd_codes']]  # Filter relevant columns\n",
        "    mimic_data.dropna(inplace=True)  # Drop rows with missing values\n",
        "    train_data, val_data, test_data = split_data(mimic_data)  # Split the data into train, validation, and test sets\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "Gbv2V7coFbcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Hyperbolic Embedding Training\n",
        "def train_hyperbolic_embedding(icd_hierarchy):\n",
        "    poincare_model = PoincareModel()\n",
        "    poincare_optimizer = RiemannianAdam(poincare_model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs_poincare = 5\n",
        "    for epoch in range(num_epochs_poincare):\n",
        "        poincare_model.train()\n",
        "        for icd_codes in icd_hierarchy:\n",
        "            poincare_optimizer.zero_grad()\n",
        "            poincare_loss = poincare_model.loss(icd_codes)\n",
        "            poincare_loss.backward()\n",
        "            poincare_optimizer.step()"
      ],
      "metadata": {
        "id": "vp8C4ISOFfW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Define multi-label classification model\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_labels):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.fc = nn.Linear(bert_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # Use the [CLS] token representation\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "5YsmAwydEUsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4QqAcA9FZdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 3: BERT-based Representation\n",
        "def get_bert_embeddings(texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "    return embeddings\n",
        "\n",
        "# Step 4: Integration and Multi-label Classification\n",
        "class HyperBertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, poincare_model, num_classes):\n",
        "        super(HyperBertClassifier, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.poincare_model = poincare_model\n",
        "        self.linear = nn.Linear(bert_model.config.hidden_size + poincare_model.embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_embeddings = bert_outputs.last_hidden_state[:, 0, :]\n",
        "        poincare_embeddings = self.poincare_model.get_embeddings()  # Assuming you have a method to get embeddings\n",
        "        combined_embeddings = torch.cat((bert_embeddings, poincare_embeddings), dim=1)\n",
        "        logits = self.linear(combined_embeddings)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "6UJFOObKHzoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Step 4: Integration and Multi-label Classification\n",
        "class HyperBertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, poincare_model, num_classes):\n",
        "        super(HyperBertClassifier, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.poincare_model = poincare_model\n",
        "        self.linear = nn.Linear(bert_model.config.hidden_size + poincare_model.embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_embeddings = bert_outputs.last_hidden_state[:, 0, :]\n",
        "        poincare_embeddings = self.poincare_model.get_embeddings()  # Assuming you have a method to get embeddings\n",
        "        combined_embeddings = torch.cat((bert_embeddings, poincare_embeddings), dim=1)\n",
        "        logits = self.linear(combined_embeddings)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "rxIZvZ7jFhri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 5: Model Architecture\n",
        "class HyperBertModel(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(HyperBertModel, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.fc = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = self.fc(bert_output.pooler_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aTA5pu1QFq1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Model Training\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "nmOVdGdUFyMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluation\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    return all_labels, all_preds"
      ],
      "metadata": {
        "id": "FOhSZB94F3Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation option\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "predictions, true_labels = evaluate_model(classifier_model, test_loader, device)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1_micro = f1_score(true_labels, (predictions >= 0.5).astype(int), average='micro')\n",
        "f1_macro = f1_score(true_labels, (predictions >= 0.5).astype(int), average='macro')\n",
        "\n",
        "# Compute AUC score\n",
        "auc_score = roc_auc_score(true_labels, predictions)\n",
        "\n",
        "print(f'Micro F1-score: {f1_micro}')\n",
        "print(f'Macro F1-score: {f1_macro}')\n",
        "print(f'AUC Score: {auc_score}')\n",
        "\n",
        "# Compute document-code similarity\n",
        "# Assuming bert_embeddings and hyperbolic_embeddings are computed\n",
        "similarity_scores = compute_similarity(bert_embeddings, hyperbolic_embeddings)\n",
        "\n",
        "# Utilize model interpretability techniques (e.g., attention and visualization) for code-aware document representations\n",
        "# Code for attention visualization and interpretation....#"
      ],
      "metadata": {
        "id": "a5TemVuzIDoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Step 8: Fine-tuning Process\n",
        "def fine_tune_model(model, fine_tune_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch in fine_tune_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(fine_tune_loader.dataset)\n",
        "        print(f\"Fine-tuning Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "u8pnY3bFF7rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Document-Code Similarity Prediction (if needed)\n",
        "def compute_similarity(document_embedding, code_embedding):\n",
        "    # Compute similarity in hyperbolic space\n",
        "    similarity_score = ...\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "QMBJyK-dIrmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Code-wise Label Attention Visualization\n",
        "def visualize_attention(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.bert_model(input_ids, attention_mask)\n",
        "        attentions = outputs['attentions'][-1]  # Get attention weights from the last layer\n",
        "    # Implement your attention visualization code here\n",
        "    return attentions"
      ],
      "metadata": {
        "id": "BluS-pn3F_qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main code\n",
        "# Assuming train_loader, fine_tune_loader, and test_loader are DataLoader objects containing training, fine-tuning, and test data respectively\n",
        "train_hyperbolic_embedding(icd_hierarchy)\n",
        "train_data, _, _ = preprocess_mimic_iv('your_dataset.csv')\n",
        "train_dataset = MultiLabelDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HyperBertModel(bert_model).to(device)\n",
        "optimizer = RiemannianAdam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "# After training, fine-tune the model for ICD code hierarchy\n",
        "fine_tune_model(model, fine_tune_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "# Evaluate the model\n",
        "test_data, _, _ = preprocess_mimic_iv('your_test_dataset.csv')\n",
        "test_dataset = MultiLabelDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "labels, preds = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Compute document-code similarity predictions and visualize code-wise label attention\n",
        "text = \"Your discharge summary text\"\n",
        "attentions = visualize_attention(model, tokenizer, text)"
      ],
      "metadata": {
        "id": "-1IBh1ydGD37"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}